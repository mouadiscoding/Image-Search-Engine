{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "work_dir = os.getcwd()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(work_dir,'train')       #Path of train directory. \n",
    "test_dir = os.path.join(work_dir,'test')         #Path of test directory.\n",
    "os.makedirs(train_dir, exist_ok=True)  #Creating a directory for training examples. \n",
    "os.makedirs(test_dir, exist_ok=True)   #Creating a directory for testing examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of Classes in the Dataset is:105\n"
     ]
    }
   ],
   "source": [
    "train_ratio=0.8\n",
    "test_ratio=0.2\n",
    "dir_list=os.listdir(os.path.join(work_dir ,'105_classes_pins_dataset'))\n",
    "print('The Number of Classes in the Dataset is:{}'.format(len(dir_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "import numpy as np\n",
    "\n",
    "source_dir = os.path.join(work_dir ,'105_classes_pins_dataset')        #Unzipped dataset directory \n",
    "\n",
    "dir_list = os.listdir(source_dir)\n",
    "\n",
    "for folder in dir_list:\n",
    "    data_dir = os.listdir(os.path.join(source_dir,folder))\n",
    "    np.random.shuffle(data_dir)                            #Shuffling the examples of the classes inside the dataset.\n",
    "    os.makedirs(os.path.join(train_dir , folder), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir , folder), exist_ok=True)\n",
    "    train_data = data_dir[:int(len(data_dir)*train_ratio+1)] #Splitting the training dataset with respect to the train_ratio.\n",
    "    test_data = data_dir[-int(len(data_dir)*test_ratio):]\n",
    "  \n",
    "    for image in train_data:\n",
    "        copyfile(os.path.join(source_dir,folder,image) , os.path.join(train_dir,folder,image)) #Copying the Training files from dataset to training directory.\n",
    "    \n",
    "    for image in test_data:\n",
    "       copyfile(os.path.join(source_dir,folder,image) , os.path.join(test_dir,folder,image))  #Copying the Training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of Classes in the Training Set:105\n",
      "The Number Of Classes in the Testing Set:105\n",
      "For Verification of the Split.....\n",
      "The Number of Examples in the Class='pins_Katherine Langford' in the Dataset Directory:226\n",
      "The Number of Examples in the Class='pins_Katherine Langford' in the Train Directory:216\n",
      "The Number of Examples in the Class='pins_Katherine Langford' in the Test Directory:80\n"
     ]
    }
   ],
   "source": [
    "print(\"The Number of Classes in the Training Set:{}\".format(len(os.listdir(train_dir))))\n",
    "print(\"The Number Of Classes in the Testing Set:{}\".format(len(os.listdir(test_dir))))\n",
    "print('For Verification of the Split.....')\n",
    "print(\"The Number of Examples in the Class='pins_Katherine Langford' in the Dataset Directory:{}\".format(len(os.listdir(os.path.join(source_dir,'pins_Katherine Langford')))))\n",
    "print(\"The Number of Examples in the Class='pins_Katherine Langford' in the Train Directory:{}\".format(len(os.listdir(os.path.join(train_dir,'pins_Katherine Langford')))))\n",
    "print(\"The Number of Examples in the Class='pins_Katherine Langford' in the Test Directory:{}\".format(len(os.listdir(os.path.join(test_dir,'pins_Katherine Langford')))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "rows=224  #Number of Pixels in the Rows for Input. \n",
    "cols=224 #Number of Pixels in Columns for Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "# import metric\n",
    "from keras.metrics import categorical_crossentropy\n",
    "# optimization method\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def AlexNet(input_shape=(rows,cols,3), output_shape=105):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Conv2D(96, (11, 11), strides=(4, 4), padding='valid', activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "\n",
    "    # Layers 3-5\n",
    "    model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "\n",
    "    # Layers 6-8\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    opt = SGD(lr=0.01)  # Changed learning rate for better convergence\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 54, 54, 96)        34944     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 26, 26, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 256)       614656    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 384)       885120    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 384)       1327488   \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 256)       884992    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 5, 5, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              26218496  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 105)               430185    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,177,193\n",
      "Trainable params: 47,177,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\imoua\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16930 images belonging to 105 classes.\n",
      "Found 6638 images belonging to 105 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255,\n",
    "                                 shear_range=0.2,\n",
    "                                 zoom_range=0.2,\n",
    "                                 horizontal_flip=True,\n",
    "                                 rotation_range=40,\n",
    "                                 width_shift_range=0.1,\n",
    "                                 height_shift_range=0.1)\n",
    "                                 \n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(train_dir,\n",
    "                                                  target_size=(rows,cols),\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "test_generator=test_datagen.flow_from_directory(test_dir,\n",
    "                                                target_size=(rows,cols),\n",
    "                                                class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "# Create a function to build a TensorBoard callback\n",
    "def create_tensorboard_callback():\n",
    "  # Create a log directory for storing TensorBoard logs\n",
    "  logdir = os.path.join(\"./logs\",\n",
    "                        # Make it so the logs get tracked whenever we run an experiment\n",
    "                        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "  return tf.keras.callbacks.TensorBoard(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create early stopping (once our model stops improving, stop training)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",\n",
    "                                                  patience=3) # stops after 3 rounds of no improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100 #@param {type:\"slider\", min:10, max:100, step:10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function to train and return a trained model\n",
    "def train_model():\n",
    "  \"\"\"\n",
    "  Trains a given model and returns the trained version.\n",
    "  \"\"\"\n",
    "  # Create a model\n",
    "  model = AlexNet()\n",
    "\n",
    "  # Create new TensorBoard session everytime we train a model\n",
    "  tensorboard = create_tensorboard_callback()\n",
    "\n",
    "  # Fit the model to the data passing it the callbacks we created\n",
    "  model.fit(x=train_generator,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            validation_data=test_generator,\n",
    "            validation_freq=1, # check validation metrics every epoch\n",
    "            callbacks=[tensorboard, early_stopping])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "441/441 [==============================] - 163s 367ms/step - loss: 4.6501 - accuracy: 0.0110 - val_loss: 4.6373 - val_accuracy: 0.0127\n",
      "Epoch 2/100\n",
      "441/441 [==============================] - 118s 266ms/step - loss: 4.6410 - accuracy: 0.0120 - val_loss: 4.6321 - val_accuracy: 0.0136\n",
      "Epoch 3/100\n",
      "441/441 [==============================] - 116s 262ms/step - loss: 4.6356 - accuracy: 0.0136 - val_loss: 4.6246 - val_accuracy: 0.0179\n",
      "Epoch 4/100\n",
      "441/441 [==============================] - 117s 265ms/step - loss: 4.6323 - accuracy: 0.0122 - val_loss: 4.6223 - val_accuracy: 0.0127\n",
      "Epoch 5/100\n",
      "441/441 [==============================] - 117s 266ms/step - loss: 4.6281 - accuracy: 0.0147 - val_loss: 4.6184 - val_accuracy: 0.0188\n",
      "Epoch 6/100\n",
      "441/441 [==============================] - 119s 270ms/step - loss: 4.6221 - accuracy: 0.0121 - val_loss: 4.6110 - val_accuracy: 0.0199\n",
      "Epoch 7/100\n",
      "441/441 [==============================] - 116s 262ms/step - loss: 4.6152 - accuracy: 0.0141 - val_loss: 4.5927 - val_accuracy: 0.0231\n",
      "Epoch 8/100\n",
      "441/441 [==============================] - 111s 252ms/step - loss: 4.6061 - accuracy: 0.0180 - val_loss: 4.5767 - val_accuracy: 0.0289\n",
      "Epoch 9/100\n",
      "441/441 [==============================] - 114s 258ms/step - loss: 4.5915 - accuracy: 0.0209 - val_loss: 4.5412 - val_accuracy: 0.0266\n",
      "Epoch 10/100\n",
      "441/441 [==============================] - 115s 262ms/step - loss: 4.5665 - accuracy: 0.0225 - val_loss: 4.5303 - val_accuracy: 0.0295\n",
      "Epoch 11/100\n",
      "441/441 [==============================] - 115s 261ms/step - loss: 4.5242 - accuracy: 0.0268 - val_loss: 4.4003 - val_accuracy: 0.0367\n",
      "Epoch 12/100\n",
      "441/441 [==============================] - 119s 269ms/step - loss: 4.4762 - accuracy: 0.0286 - val_loss: 4.3425 - val_accuracy: 0.0381\n",
      "Epoch 13/100\n",
      "441/441 [==============================] - 117s 265ms/step - loss: 4.4368 - accuracy: 0.0309 - val_loss: 4.3549 - val_accuracy: 0.0479\n",
      "Epoch 14/100\n",
      "441/441 [==============================] - 117s 265ms/step - loss: 4.3975 - accuracy: 0.0363 - val_loss: 4.2208 - val_accuracy: 0.0520\n",
      "Epoch 15/100\n",
      "441/441 [==============================] - 119s 270ms/step - loss: 4.3434 - accuracy: 0.0417 - val_loss: 4.1511 - val_accuracy: 0.0592\n",
      "Epoch 16/100\n",
      "441/441 [==============================] - 114s 257ms/step - loss: 4.2983 - accuracy: 0.0481 - val_loss: 4.0930 - val_accuracy: 0.0612\n",
      "Epoch 17/100\n",
      "441/441 [==============================] - 113s 256ms/step - loss: 4.2426 - accuracy: 0.0500 - val_loss: 4.0435 - val_accuracy: 0.0722\n",
      "Epoch 18/100\n",
      "441/441 [==============================] - 115s 260ms/step - loss: 4.1831 - accuracy: 0.0538 - val_loss: 3.9903 - val_accuracy: 0.0725\n",
      "Epoch 19/100\n",
      "441/441 [==============================] - 113s 256ms/step - loss: 4.1278 - accuracy: 0.0579 - val_loss: 3.8852 - val_accuracy: 0.0881\n",
      "Epoch 20/100\n",
      "441/441 [==============================] - 115s 260ms/step - loss: 4.0738 - accuracy: 0.0626 - val_loss: 3.8160 - val_accuracy: 0.0889\n",
      "Epoch 21/100\n",
      "441/441 [==============================] - 118s 267ms/step - loss: 4.0165 - accuracy: 0.0704 - val_loss: 3.7434 - val_accuracy: 0.0947\n",
      "Epoch 22/100\n",
      "441/441 [==============================] - 114s 257ms/step - loss: 3.9484 - accuracy: 0.0745 - val_loss: 3.6734 - val_accuracy: 0.1063\n",
      "Epoch 23/100\n",
      "441/441 [==============================] - 112s 254ms/step - loss: 3.8887 - accuracy: 0.0815 - val_loss: 3.6157 - val_accuracy: 0.1077\n",
      "Epoch 24/100\n",
      "441/441 [==============================] - 114s 259ms/step - loss: 3.8336 - accuracy: 0.0839 - val_loss: 3.5379 - val_accuracy: 0.1161\n",
      "Epoch 25/100\n",
      "441/441 [==============================] - 113s 255ms/step - loss: 3.7753 - accuracy: 0.0949 - val_loss: 3.4562 - val_accuracy: 0.1375\n",
      "Epoch 26/100\n",
      "441/441 [==============================] - 115s 262ms/step - loss: 3.7185 - accuracy: 0.1011 - val_loss: 3.4250 - val_accuracy: 0.1299\n",
      "Epoch 27/100\n",
      "441/441 [==============================] - 114s 259ms/step - loss: 3.6591 - accuracy: 0.1049 - val_loss: 3.4017 - val_accuracy: 0.1369\n",
      "Epoch 28/100\n",
      "441/441 [==============================] - 117s 265ms/step - loss: 3.5970 - accuracy: 0.1168 - val_loss: 3.2795 - val_accuracy: 0.1652\n",
      "Epoch 29/100\n",
      "441/441 [==============================] - 111s 252ms/step - loss: 3.5366 - accuracy: 0.1277 - val_loss: 3.2123 - val_accuracy: 0.1811\n",
      "Epoch 30/100\n",
      "441/441 [==============================] - 115s 261ms/step - loss: 3.4771 - accuracy: 0.1409 - val_loss: 3.1428 - val_accuracy: 0.1868\n",
      "Epoch 31/100\n",
      "441/441 [==============================] - 112s 255ms/step - loss: 3.4060 - accuracy: 0.1483 - val_loss: 3.0963 - val_accuracy: 0.2016\n",
      "Epoch 32/100\n",
      "441/441 [==============================] - 115s 260ms/step - loss: 3.3344 - accuracy: 0.1661 - val_loss: 2.9597 - val_accuracy: 0.2336\n",
      "Epoch 33/100\n",
      "441/441 [==============================] - 112s 254ms/step - loss: 3.2632 - accuracy: 0.1788 - val_loss: 2.9377 - val_accuracy: 0.2420\n",
      "Epoch 34/100\n",
      "441/441 [==============================] - 116s 263ms/step - loss: 3.2143 - accuracy: 0.1936 - val_loss: 2.8083 - val_accuracy: 0.2622\n",
      "Epoch 35/100\n",
      "441/441 [==============================] - 113s 257ms/step - loss: 3.1300 - accuracy: 0.2035 - val_loss: 2.8966 - val_accuracy: 0.2377\n",
      "Epoch 36/100\n",
      "441/441 [==============================] - 114s 259ms/step - loss: 3.0698 - accuracy: 0.2201 - val_loss: 2.6287 - val_accuracy: 0.3145\n",
      "Epoch 37/100\n",
      "441/441 [==============================] - 111s 252ms/step - loss: 2.9921 - accuracy: 0.2308 - val_loss: 2.6425 - val_accuracy: 0.2986\n",
      "Epoch 38/100\n",
      "441/441 [==============================] - 114s 259ms/step - loss: 2.9310 - accuracy: 0.2452 - val_loss: 2.5742 - val_accuracy: 0.3228\n",
      "Epoch 39/100\n",
      "441/441 [==============================] - 111s 252ms/step - loss: 2.8688 - accuracy: 0.2567 - val_loss: 2.4475 - val_accuracy: 0.3509\n",
      "Epoch 40/100\n",
      "441/441 [==============================] - 115s 260ms/step - loss: 2.7979 - accuracy: 0.2750 - val_loss: 2.3557 - val_accuracy: 0.3679\n",
      "Epoch 41/100\n",
      "441/441 [==============================] - 112s 253ms/step - loss: 2.7503 - accuracy: 0.2780 - val_loss: 2.4388 - val_accuracy: 0.3471\n",
      "Epoch 42/100\n",
      "441/441 [==============================] - 116s 263ms/step - loss: 2.6916 - accuracy: 0.2944 - val_loss: 2.2788 - val_accuracy: 0.3971\n",
      "Epoch 43/100\n",
      "441/441 [==============================] - 111s 252ms/step - loss: 2.6235 - accuracy: 0.3065 - val_loss: 2.2759 - val_accuracy: 0.3832\n",
      "Epoch 44/100\n",
      "441/441 [==============================] - 115s 262ms/step - loss: 2.5604 - accuracy: 0.3209 - val_loss: 2.1571 - val_accuracy: 0.4225\n",
      "Epoch 45/100\n",
      "441/441 [==============================] - 113s 256ms/step - loss: 2.5351 - accuracy: 0.3315 - val_loss: 2.2327 - val_accuracy: 0.3994\n",
      "Epoch 46/100\n",
      "441/441 [==============================] - 114s 258ms/step - loss: 2.4648 - accuracy: 0.3446 - val_loss: 2.1140 - val_accuracy: 0.4254\n",
      "Epoch 47/100\n",
      "441/441 [==============================] - 111s 252ms/step - loss: 2.3948 - accuracy: 0.3618 - val_loss: 1.9573 - val_accuracy: 0.4658\n",
      "Epoch 48/100\n",
      "441/441 [==============================] - 114s 258ms/step - loss: 2.3578 - accuracy: 0.3703 - val_loss: 1.9990 - val_accuracy: 0.4600\n",
      "Epoch 49/100\n",
      "441/441 [==============================] - 113s 257ms/step - loss: 2.2835 - accuracy: 0.3864 - val_loss: 1.9658 - val_accuracy: 0.4632\n",
      "Epoch 50/100\n",
      "441/441 [==============================] - 117s 265ms/step - loss: 2.2446 - accuracy: 0.3950 - val_loss: 1.9098 - val_accuracy: 0.4811\n",
      "Epoch 51/100\n",
      "441/441 [==============================] - 114s 258ms/step - loss: 2.1780 - accuracy: 0.4107 - val_loss: 1.9332 - val_accuracy: 0.4912\n",
      "Epoch 52/100\n",
      "441/441 [==============================] - 114s 259ms/step - loss: 2.1306 - accuracy: 0.4184 - val_loss: 1.7658 - val_accuracy: 0.5290\n",
      "Epoch 53/100\n",
      "441/441 [==============================] - 112s 255ms/step - loss: 2.0717 - accuracy: 0.4362 - val_loss: 1.7139 - val_accuracy: 0.5388\n",
      "Epoch 54/100\n",
      "441/441 [==============================] - 116s 263ms/step - loss: 2.0320 - accuracy: 0.4432 - val_loss: 1.8566 - val_accuracy: 0.4975\n",
      "Epoch 55/100\n",
      "441/441 [==============================] - 115s 260ms/step - loss: 1.9756 - accuracy: 0.4637 - val_loss: 1.6380 - val_accuracy: 0.5458\n",
      "Epoch 56/100\n",
      "441/441 [==============================] - 122s 276ms/step - loss: 1.9689 - accuracy: 0.4609 - val_loss: 1.6314 - val_accuracy: 0.5634\n",
      "Epoch 57/100\n",
      "441/441 [==============================] - 119s 270ms/step - loss: 1.8908 - accuracy: 0.4772 - val_loss: 1.6173 - val_accuracy: 0.5694\n",
      "Epoch 58/100\n",
      "441/441 [==============================] - 124s 281ms/step - loss: 1.8447 - accuracy: 0.4925 - val_loss: 1.5460 - val_accuracy: 0.5746\n",
      "Epoch 59/100\n",
      "441/441 [==============================] - 123s 278ms/step - loss: 1.7987 - accuracy: 0.5008 - val_loss: 1.5298 - val_accuracy: 0.5824\n",
      "Epoch 60/100\n",
      "441/441 [==============================] - 121s 275ms/step - loss: 1.7256 - accuracy: 0.5208 - val_loss: 1.4569 - val_accuracy: 0.6055\n",
      "Epoch 61/100\n",
      "441/441 [==============================] - 120s 273ms/step - loss: 1.7105 - accuracy: 0.5227 - val_loss: 1.4664 - val_accuracy: 0.6076\n",
      "Epoch 62/100\n",
      "441/441 [==============================] - 122s 276ms/step - loss: 1.6746 - accuracy: 0.5329 - val_loss: 1.4207 - val_accuracy: 0.6012\n",
      "Epoch 63/100\n",
      "441/441 [==============================] - 118s 268ms/step - loss: 1.6343 - accuracy: 0.5435 - val_loss: 1.4874 - val_accuracy: 0.6006\n",
      "Epoch 64/100\n",
      "441/441 [==============================] - 125s 283ms/step - loss: 1.5912 - accuracy: 0.5540 - val_loss: 1.4000 - val_accuracy: 0.6157\n",
      "Epoch 65/100\n",
      "441/441 [==============================] - 125s 283ms/step - loss: 1.5592 - accuracy: 0.5621 - val_loss: 1.3435 - val_accuracy: 0.6255\n",
      "Epoch 66/100\n",
      "441/441 [==============================] - 124s 282ms/step - loss: 1.4927 - accuracy: 0.5765 - val_loss: 1.3230 - val_accuracy: 0.6385\n",
      "Epoch 67/100\n",
      "441/441 [==============================] - 125s 283ms/step - loss: 1.4666 - accuracy: 0.5831 - val_loss: 1.2850 - val_accuracy: 0.6465\n",
      "Epoch 68/100\n",
      "441/441 [==============================] - 125s 282ms/step - loss: 1.4438 - accuracy: 0.5913 - val_loss: 1.3094 - val_accuracy: 0.6382\n",
      "Epoch 69/100\n",
      "441/441 [==============================] - 124s 282ms/step - loss: 1.4074 - accuracy: 0.6017 - val_loss: 1.2892 - val_accuracy: 0.6486\n",
      "Epoch 70/100\n",
      "441/441 [==============================] - 125s 284ms/step - loss: 1.3583 - accuracy: 0.6159 - val_loss: 1.2268 - val_accuracy: 0.6610\n",
      "Epoch 71/100\n",
      "441/441 [==============================] - 124s 281ms/step - loss: 1.3374 - accuracy: 0.6198 - val_loss: 1.2340 - val_accuracy: 0.6688\n",
      "Epoch 72/100\n",
      "441/441 [==============================] - 125s 283ms/step - loss: 1.3066 - accuracy: 0.6269 - val_loss: 1.1401 - val_accuracy: 0.6806\n",
      "Epoch 73/100\n",
      "441/441 [==============================] - 125s 282ms/step - loss: 1.2629 - accuracy: 0.6362 - val_loss: 1.2068 - val_accuracy: 0.6665\n",
      "Epoch 74/100\n",
      "441/441 [==============================] - 144s 327ms/step - loss: 1.2297 - accuracy: 0.6467 - val_loss: 1.1531 - val_accuracy: 0.6806\n",
      "Epoch 75/100\n",
      "441/441 [==============================] - 138s 313ms/step - loss: 1.2251 - accuracy: 0.6459 - val_loss: 1.1394 - val_accuracy: 0.6884\n",
      "Epoch 76/100\n",
      "441/441 [==============================] - 114s 259ms/step - loss: 1.1859 - accuracy: 0.6595 - val_loss: 1.1322 - val_accuracy: 0.6864\n",
      "Epoch 77/100\n",
      "441/441 [==============================] - 116s 262ms/step - loss: 1.1677 - accuracy: 0.6659 - val_loss: 1.0651 - val_accuracy: 0.7185\n",
      "Epoch 78/100\n",
      "441/441 [==============================] - 121s 273ms/step - loss: 1.1390 - accuracy: 0.6711 - val_loss: 1.1048 - val_accuracy: 0.7055\n",
      "Epoch 79/100\n",
      "441/441 [==============================] - 131s 297ms/step - loss: 1.1133 - accuracy: 0.6754 - val_loss: 1.0342 - val_accuracy: 0.7179\n",
      "Epoch 80/100\n",
      "441/441 [==============================] - 130s 295ms/step - loss: 1.0800 - accuracy: 0.6851 - val_loss: 1.0660 - val_accuracy: 0.7066\n"
     ]
    }
   ],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./models/faces_classication.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"./models/faces_classication.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "The Person in the Image is Lionel Messi .\n"
     ]
    }
   ],
   "source": [
    "path=\"./image/messi.jpg\"  #Path to the target image to be predicted. \n",
    "\n",
    "classes=train_generator.class_indices\n",
    "class_names=list(classes.keys())   #List of the class names\n",
    "img = tf.keras.preprocessing.image.load_img(\n",
    "    path, target_size=(rows, cols)\n",
    ")\n",
    "\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "img_array=img_array/255.\n",
    "\n",
    "score = model.predict(img_array)\n",
    "\n",
    "print(\n",
    "    \"The Person in the Image is {} .\"\n",
    "    .format(class_names[np.argmax(score)][5:].title())\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
